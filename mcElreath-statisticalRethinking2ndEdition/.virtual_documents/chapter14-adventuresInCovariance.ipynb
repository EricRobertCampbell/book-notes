


# setting up the environment
library(rethinking)
library(MASS)
library(ggplot2)
library(dagitty)
library(ape)

options(repr.plot.width = 16, repr.plot.height = 8)





a <- 3.5 # average morning wait times
b <- -1 # average difference in the afternoon wait times
sigma_a <- 1 # std dev in intercepts
sigma_b <- 0.5 # std dev in slopes
rho <- -0.7 # correlation between intercepts and slopes





Mu <- c(a, b)





cov_ab <- sigma_a * sigma_b * rho
Sigma <- matrix(c(sigma_a ^ 2, cov_ab, sigma_b ^ 2, cov_ab), ncol = 2)





sigmas <- c(sigma_a, sigma_b) # standard deviations
Rho <- matrix(c(1, rho, rho, 1), nrow = 2) # correlation matrix

# matrix multiply to get the covariance matrix
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)





diag(c(1, 2, 3))





N_cafes <- 20

set.seed(5)
vary_effects <- mvrnorm(N_cafes, Mu, Sigma)
vary_effects





a_cafe <- vary_effects[, 1]
b_cafe <- vary_effects[, 2]


plot_df <- data.frame(
    intercept = a_cafe,
    slope = b_cafe
)

ggplot(plot_df, aes(intercept, slope)) +
    geom_point() +
    geom_density2d()





set.seed(22)
N_visits <- 10
afternoon <- rep(0:1, N_visits * N_cafes / 2)
cafe_id <- rep(1:N_cafes, each = N_visits)
mu <- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
sigma <- 0.5 # sd within cafes
wait <- rnorm(N_visits * N_cafes, mu, sigma)
d <- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)





R2 <- rlkjcorr(1e4, K = 2, eta = 2)
dims(R2)


length(R2)


R2[1, , ]





R1 <- rlkjcorr(1e4, K = 2, eta = 1)
R4 <- rlkjcorr(1e4, K = 2, eta = 4)

plot_df <- rbind(
    data.frame(rho = R1[, 1, 2], eta = 1),
    data.frame(rho = R2[, 1, 2], eta = 2),
    data.frame(rho = R4[, 1, 2], eta = 4)
)
plot_df$eta <- as.factor(plot_df$eta)
ggplot(plot_df, aes(rho, group = eta, colour = eta)) +
    geom_density()


# now let's fit the model!
set.seed(867530)
m14.1 <- ulam(
    alist(
        wait ~ normal(mu, sigma),
        mu <- a_cafe[cafe] + b_cafe[cafe] * afternoon,
        c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
        a ~ normal(5, 2),
        b ~ normal(-1, 0.5),
        sigma_cafe ~ exponential(1),
        sigma ~ exponential(1),
        Rho ~ lkj_corr(2)
    ),
    data = d,
    chains = 4,
    cores = 4
)





stancode(m14.1)





post <- extract.samples(m14.1)

prior_rho <- rlkjcorr(1e4, K = 2, eta = 2)

plot_df <- rbind(
    data.frame(points = post$Rho[, 1, 2], type = 'Posterior'),
    data.frame(points = prior_rho[, 1, 2], type = 'Prior')
)
ggplot(plot_df, aes(points, colour = type, group = type)) + 
    geom_density() + 
    scale_x_continuous(limits = c(-1, 1))





# unpooled estimates directly from the data
a1 <- sapply(1:N_cafes, function(i) mean(wait[cafe_id == i & afternoon == 0])) # intercept = morning wait time
b1 <- sapply(1:N_cafes, function(i) mean(wait[cafe_id == i & afternoon == 1])) - a1 # afternoon - morning

# extract the posterior means of the partially pooled estimates
post <- extract.samples(m14.1)
a2 <- apply(post$a_cafe, 2, mean)
b2 <- apply(post$b_cafe, 2, mean)

# getting the levels of the multivariate normal
Mu_est <- c(mean(post$a), mean(post$b))
rho_est <- mean(post$Rho[, 1, 2])
sa_est <- mean(post$sigma_cafe[, 1])
sb_est <- mean(post$sigma_cafe[, 2])
cov_ab <- sa_est * sb_est * rho_est
Sigma_est <- matrix(c(sa_est ^ 2, cov_ab, cov_ab, sb_est^2), ncol = 2)
prob_a <- seq(min(a2) - 1, max(a2) + 1, by = 0.1)
prob_b <- seq(min(b2) - 1, max(b2) + 1, by = 0.1)
prob_grid <- expand.grid(prob_a, prob_b)
probs <- dmvnorm(prob_grid, mean = Mu_est, sigma = Sigma_est)
probs_df <- data.frame(
    a = prob_grid[, 1],
    b = prob_grid[, 2],
    prob = probs
)

plot_df <- rbind(
    data.frame(a = a1, b = b1, type = "Raw"),
    data.frame(a = a2, b = b2, type = "Posterior")
)
ggplot(plot_df, aes(a, b)) +
    geom_point(aes(group = type, colour = type)) +
    geom_segment(data = data.frame(x = a1, y = b1, xend = a2, yend = b2), mapping = aes(x = x, y = y, xend = xend, yend = yend), arrow = arrow()) +
    geom_contour(data = probs_df, mapping = aes(z = prob), bins = 5) +
    labs(x = "Intercept", y = "Slope")





# convert varying effects to wait times
wait_morning_1 <- a1
wait_afternoon_1 <- a1 + b1
wait_morning_2 <- a2
wait_afternoon_2 <- a2 + b2

lines_df <- data.frame(
    wait_morning_1 = wait_morning_1,
    wait_afternoon_1 = wait_afternoon_1,
    wait_morning_2 = wait_morning_2,
    wait_afternoon_2 = wait_afternoon_2
)
plot_df <- rbind(
    data.frame(morning = wait_morning_1, afternoon = wait_afternoon_1, type = "Raw"),
    data.frame(morning = wait_morning_2, afternoon = wait_afternoon_2, type = "Posterior")
)

p <- ggplot(plot_df, aes(morning, afternoon)) +
    geom_point(aes(group = type, colour = type)) +
    geom_segment(data = lines_df, mapping = aes(x = wait_morning_1, y = wait_afternoon_1, xend = wait_morning_2, yend = wait_afternoon_2), arrow = arrow())
print(p)


# now the shrinkage
v <- mvrnorm(1e4, Mu_est, Sigma_est)
v[, 2] <- v[, 1] + v[, 2] # calculating the afternoon wait
Sigma_est2 <- cov(v)
Mu_est2 <- Mu_est
Mu_est2[2] <- Mu_est[1] + Mu_est[2]

prob_a <- seq(1, 6, by = 0.1)
prob_b <- seq(0, 6, by = 0.1)
grid <- expand.grid(prob_a, prob_b)
probs <- dmvnorm(grid, Mu_est2, Sigma_est2)

probs_df <- data.frame(morning = grid[, 1], afternoon = grid[, 2], z = probs)

p + geom_contour(data = probs_df, mapping = aes(morning, afternoon, z = z))





data(chimpanzees)
d <- chimpanzees
d$block_id <- d$block
d$treatment <- 1L + d$prosoc_left + 2L * d$condition

data <- list(
    L = d$pulled_left,
    tid = d$treatment,
    actor = d$actor,
    block_id = as.integer(d$block_id)
)

set.seed(4387510)

m14.2 <- ulam(
    alist(
        L ~ dbinom(1, p),
        logit(p) <- g[tid] + alpha[actor, tid] + beta[block_id, tid],

        # adaptive priors
        vector[4]:alpha[actor] ~ multi_normal(0, Rho_actor, sigma_actor),
        vector[4]:beta[block_id] ~ multi_normal(0, Rho_block, sigma_block),

        # fixed priors
        g[tid] ~ dnorm(0, 1),
        sigma_actor ~ dexp(1),
        Rho_actor ~ dlkjcorr(4),
        sigma_block ~ dexp(1),
        Rho_block ~ dlkjcorr(4)
    ),
    data = data,
    chains = 4,
    cores = 4
)


par(bg = 'white')
trankplot(m14.2)


par(bg = 'white')
traceplot(m14.2)





set.seed(4387510)

m14.3 <- ulam(
    alist(
        L ~ dbinom(1, p),
        logit(p) <- g[tid] + alpha[actor, tid] + beta[block_id, tid],

        # adaptive priors - non-centred
        # vector[4]:alpha[actor] ~ multi_normal(0, Rho_actor, sigma_actor),
        # vector[4]:beta[block_id] ~ multi_normal(0, Rho_block, sigma_block),
        transpars> matrix[actor, 4]:alpha <- compose_noncentered(sigma_actor, L_Rho_actor, z_actor),
        transpars> matrix[block_id,4]:beta <- compose_noncentered(sigma_block, L_Rho_block, z_block),
        matrix[4,actor]:z_actor ~ normal(0, 1),
        matrix[4,block_id]:z_block ~ normal(0, 1),

        # fixed priors
        g[tid] ~ dnorm(0, 1),
        # sigma_actor ~ dexp(1),
        vector[4]:sigma_actor ~ dexp(1),
        # Rho_actor ~ dlkjcorr(4),
        cholesky_factor_corr[4]:L_Rho_actor ~ lkj_corr_cholesky(2),
        # sigma_block ~ dexp(1),
        vector[4]:sigma_block ~ dexp(1),
        cholesky_factor_corr[4]:L_Rho_block ~ lkj_corr_cholesky(2),
        # Rho_block ~ dlkjcorr(4)

        # compute ordinary correlation matrices from Cholesky factors
        gq> matrix[4,4]:Rho_actor <<- Chol_to_Corr(L_Rho_actor),
        gq> matrix[4,4]:Rho_block <<- Chol_to_Corr(L_Rho_block)
    ),
    data = data,
    chains = 4,
    cores = 4,
    log_lik = TRUE
)


stancode(m14.3)





precis(m14.3, depth = 2, pars = c("sigma_actor", "sigma_block"))





pl <- by(d$pulled_left, list(d$actor, d$treatment), mean)

# generate the posterior predictions using link
datp <- list(
    actor = rep(1:7, each = 4),
    tid = rep(1:4, times = 7),
    block_id = rep(5, times = 4 * 7)
)

p_post <- link(m14.3, data = datp)
p_mu <- apply(p_post, 2, mean)
p_ci <- apply(p_post, 2, PI)

# set up the plot
par(bg = 'white')
plot(NULL, xlim = c(1, 28), ylim = c(0, 1), xlab = "", ylab = "Proportion left lever", xaxt = "n", yaxt = "n")
axis(2, at = c(0, 0.5, 1), labels = c(0, 0.5, 1))
abline(h = 0.5, lty = 2)
for (j in 1:7) {
    abline(v = (j - 1) * 4 + 4.5, lwd = 0.5)
    text((j - 1) * 4 + 2.5, 1.1, concat("actor ", j), xpd = TRUE)
}
x0 <- 0.1 # offset distance

# raw
# the [-2] removes the index of 2 (so this is 1, 3, 4, 5, ...)
for (j in (1:7)[-2]) {
    lines((j - 1) * 4 + c(1, 3) - x0, pl[j, c(1, 3)], lwd = 2, col = rangi2)
    lines((j - 1) * 4 + c(2, 4) - x0, pl[j, c(2, 4)], lwd = 2, col = rangi2)
}
points(1:28 - x0, t(pl), pch = 16, col = 'white', cex = 1.7)
points(1:28 - x0, t(pl), pch = c(1, 1, 16, 16), col = rangi2, lwd = 2)

yoff <- 0.175
text(1 - x0, pl[1, 1] - yoff, "R/N", pos = 1, cex = 0.8)
text(1 - x0, pl[1, 2] - yoff, "L/N", pos = 3, cex = 0.8)
text(1 - x0, pl[1, 3] - yoff, "R/P", pos = 1, cex = 0.8)
text(1 - x0, pl[1, 4] - yoff, "L/P", pos = 3, cex = 0.8)

# posterior predictions
for (j in (1:7)[-2]) {
    lines( (j - 1) * 4 + c(1, 3) + x0, p_mu[(j - 1) * 4 + c(1, 3)], lwd = 2)
    lines( (j - 1) * 4 + c(2, 4) + x0, p_mu[(j - 1) * 4 + c(2, 4)], lwd = 2)
}

for (i in 1:28) {
    lines(c(i, i) + x0, p_ci[, i], lwd = 1)
}

points(1:28 + x0, p_mu, pch = 17, col = 'white', cex = 1.3)
points(1:28 + x0, p_mu, pch = c(1, 1, 16, 16))





par(bg = 'white')
dag <- dagitty('dag {
U [unobserved, pos="0,-2"]
E[pos="-1,1"]
W[pos="1,1"]
E <- U -> W
E -> W
}')
drawdag(dag)





par(bg = 'white')
dag <- dagitty('dag {
U [unobserved, pos="0,-2"]
E[pos="-1,1"]
W[pos="1,1"]
Q[pos="-2,-2"]
E <- U -> W
E -> W
Q -> E
}')
drawdag(dag)





set.seed(73)
N <- 500
U_sim <- rnorm(N)
Q_sim <- sample(1:4, size = N, replace = TRUE)
E_sim <- rnorm(N, U_sim + Q_sim)
W_sim <- rnorm(N, U_sim + 0 * E_sim)

dat_sim <- list(
    W = standardize(W_sim),
    E = standardize(E_sim),
    Q = standardize(Q_sim)
)





m14.4 <- ulam(
    alist(
        W ~ dnorm(mu, sigma),
        mu <- aW + bEW * E,
        aW ~ dnorm(0, 0.2),
        bEW ~ dnorm(0, 0.5),
        sigma ~ dexp(1)
    ),
    data = dat_sim,
    chains = 4,
    cores = 4
)
precis(m14.4)


par(bg = 'white')
plot(precis(m14.4))





m14.5 <- ulam(
    alist(
        W ~ dnorm(mu, sigma),
        mu <- aW + bEW * E + bQW * Q,
        aW ~ dnorm(0, 0.2),
        bEW ~ dnorm(0, 0.5),
        bQW ~ dnorm(0, 0.5),
        sigma ~ dexp(1)
    ),
    data = dat_sim,
    chains = 4,
    cores = 4
)
precis(m14.5)
par(bg = 'white')
plot(precis(m14.5))





m14.6 <- ulam(
    alist(
        c(W, E) ~ multi_normal(c(muW, muE), Rho, Sigma),
        muW <- aW + bEW * E,
        muE <- aE + bQE * Q,
        c(aW, aE) ~ normal(0, 0.2),
        c(bEW, bQE) ~ normal(0, 0.5),
        Rho ~ lkj_corr(2),
        Sigma ~ exponential(1)
    ),
    data = dat_sim,
    chains = 4,
    cores = 4
)
precis(m14.6, depth = 3)


par(bg = 'white')
plot(precis(m14.6, depth = 3))





U_sim <- rnorm(N)
Q_sim <- sample(1:4, size = N, replace = TRUE)
E_sim <- rnorm(N, U_sim + Q_sim)
W_sim <- rnorm(N, -U_sim + 0.2 * E_sim)
dat_sim <- list(
    W = standardize(W_sim),
    E = standardize(E_sim),
    Q = standardize(Q_sim)
)
# this re-uses the model from m14.6 but with different data, meaning we don't need to recompile
m14.6.1 <- ulam(m14.6, data = dat_sim, chains = 4, cores = 4)


par(bg = 'white')
prec <- precis(m14.6.1, depth = 3)
print(prec)
plot(prec)





dagIV <- dagitty("dag {Q -> E <- U -> W <- E}")
instrumentalVariables(dagIV, exposure = "E", outcome = "W")





dag <- dagitty('dag {
U[unobserved, pos="0, -2"]
X[pos="-1, 0"]
Y[pos="1, 0"]
Z[pos="0, 0"]
Y <- U -> X -> Z -> Y
}')
par(bg = 'white')
drawdag(dag)





data(KosterLeckie)
head(kl_dyads)
head(kl_households)





ggplot(kl_dyads, aes(giftsAB, giftsBA)) +
    geom_point()


cor.test(kl_dyads$giftsAB, kl_dyads$giftsBA)





kl_data <- list(
    N = nrow(kl_dyads),
    N_households = max(kl_dyads$hidB),
    did = kl_dyads$did,
    hidA = kl_dyads$hidA,
    hidB = kl_dyads$hidB,
    giftsAB = kl_dyads$giftsAB,
    giftsBA = kl_dyads$giftsBA
)

m14.7 <- ulam(
    alist(
        giftsAB ~ poisson(lambdaAB),
        giftsBA ~ poisson(lambdaBA),
        log(lambdaAB) <- a + gr[hidA, 1] + gr[hidB, 2] + d[did, 1],
        log(lambdaBA) <- a + gr[hidB, 1] + gr[hidA, 2] + d[did, 2],
        a ~ normal(0, 1),

        # gr matrix of the varying effects
        vector[2]: gr[N_households] ~ multi_normal(0, Rho_gr, sigma_gr),
        Rho_gr ~ lkj_corr(4),
        sigma_gr ~ exponential(1),

        # dyad effects
        transpars> matrix[N,2]:d <- compose_noncentered(rep_vector(sigma_d, 2), L_Rho_d, z),
        matrix[2, N]:z ~ normal(0, 1),
        cholesky_factor_corr[2]: L_Rho_d ~ lkj_corr_cholesky(8),
        sigma_d ~ exponential(1),

        # compute the correlation matrix for the dyads
        gq> matrix[2, 2]: Rho_d <<- Chol_to_Corr(L_Rho_d)
    ),
    data = kl_data,
    chains = 4,
    cores = 4,
    iter = 4000
)





stancode(m14.7)


precis(m14.7, depth = 3, pars=c("Rho_gr", "sigma_gr"))


par(bg = 'white')
plot(precis(m14.7, depth = 3, pars = c("Rho_gr", "sigma_gr")))





post <- extract.samples(m14.7)
g <- sapply(1:25, function(i) post$a + post$gr[, i, 1])
r <- sapply(1:25, function(i) post$a + post$gr[, i, 2])
Eg_mu <- apply(exp(g), 2, mean)
Er_mu <- apply(exp(r), 2, mean)





plot_df <- data.frame(g = Eg_mu, r = Er_mu)
p <- ggplot() +
    geom_point(data = plot_df, mapping = aes(g, r))
print(p)





exp_g <- exp(g)
exp_r <- exp(r)
uncertainty_plot <- p
for (household in 1:25) {
    gs <- exp_g[, household]
    rs <- exp_r[, household]
    Mu <- c(mean(gs), mean(rs))
    Sigma <- cov(cbind(gs, rs))
    prob_grid <- expand.grid(g = seq(mean(gs) - 2, mean(gs) + 2, by = 0.01), r = seq(mean(rs) - 2, mean(rs) + 2, by = 0.01))
    probs <- dmvnorm(prob_grid, Mu, Sigma)
    probs <- unlist(probs)
    probs_data <- data.frame(g = prob_grid[, 1], r = prob_grid[, 2], prob = probs)
    uncertainty_plot <- uncertainty_plot + geom_contour(data = probs_data, mapping = aes(g, r, z = prob), bins = 2)
}
uncertainty_plot <- uncertainty_plot + 
    geom_segment(data = data.frame(x = 0, y = 0, xend = 8, yend = 8), mapping = aes(x = x, y = y, xend = xend, yend = yend), linetype = 'dashed') +
    coord_cartesian(xlim = c(0, 8), ylim = c(0, 8))
print(uncertainty_plot)





precis(m14.7, depth = 3, pars = c("Rho_d", "sigma_d"))





dy_1 <- apply(post$d[,,1 ], 2, mean)
dy_2 <- apply(post$d[,,2 ], 2, mean)

plot_df <- data.frame(dy_1 = dy_1, dy_2 = dy_2)
ggplot(plot_df, aes(dy_1, dy_2)) +
    geom_point() +
    labs(x = "Household A", y = "Household B")





data(islandsDistMatrix)

# distances, measured in thousands of kilometres
Dmat <- islandsDistMatrix
# since this is a symmetrical matrix, we are just setting the column names to the short form of the island names...
colnames(Dmat) <- c("Ml", "Ti", "SC", "Ya", "Fi", "Tr", "Ch", "Mn", "To", "Ha")
round(Dmat, 1)





rho <- 1
linear_corr <- function(d) exp(-rho^2 * d)
quadratic_corr <- function(d) exp(-rho^2 * d^2)

distances <- seq(from = 0, to = 4, length.out = 100)
plot_df <- rbind(
    data.frame(d = distances, corr = linear_corr(distances), type = "Linear"),
    data.frame(d = distances, corr = quadratic_corr(distances), type = "Quadratic")
)

ggplot(plot_df, aes(d, corr)) +
    geom_line(aes(linetype = type))





data(Kline2)
d <- Kline2
d$society <- 1:10 # index observations
dat_list <- list(
    T = d$total_tools,
    P = d$population,
    society = d$society,
    Dmat = islandsDistMatrix
)

m14.8 <- ulam(
    alist(
        T ~ dpois(lambda),
        lambda <- (a * P ^ b / g) * exp(k[society]),
        vector[10]: k ~ multi_normal(0, SIGMA),
        matrix[10, 10]:SIGMA <- cov_GPL2(Dmat, etasq, rhosq, 0.01),
        c(a, b, g) ~ dexp(1),
        etasq ~ dexp(2),
        rhosq ~ dexp(0.5)
    ),
    data = dat_list,
    chains = 4,
    cores = 4,
    iter = 2000
)


precis(m14.8, depth = 3)





post <- extract.samples(m14.8)

x_seq <- seq(from = 0, to = 10, length.out = 100)
pmcov <- sapply(x_seq, function(x) post$etasq * exp(-post$rhosq * x^2))
pmcov_mu <- apply(pmcov, 2, mean)

plot_df <- data.frame(x = x_seq, y = pmcov_mu, type = "Mean")

p <- ggplot() +
    geom_line(data = plot_df, mapping = aes(x, y, linewidth = type))

for (i in 1:50) {
    sample_plot_df <- data.frame(
        x = x_seq,
        y = post$etasq[i] * exp(-post$rhosq[i] * x_seq ^ 2),
        type = "Sample"
    )
    p <- p + geom_line(data = sample_plot_df, mapping = aes(x, y, linewidth = type), colour = 'grey')
}
p <- p +
    scale_linewidth_manual(values = c("Mean" = 1, "Sample" = 0.25)) +
    labs(x = "distance (thousand km)", y = "covariance")
print(p)





# compute posterior median covariance
K <- matrix(0, nrow = 10, ncol = 10)
for (i in 1:10) {
    for (j in 1:10) {
        K[i, j] <- median(post$etasq) * exp(-median(post$rhosq) * islandsDistMatrix[i, j] ^ 2)
    }
}
diag(K) <- median(post$etasq) + 0.01

# now convert to a correlation matrix
Rho <- round(cov2cor(K), 2)
# column and row names
cols <- c("Ml", "Ti", "SC", "Ya", "Fi", "Tr", "Ch", "Mn", "To", "Ha")
colnames(Rho) <- cols 
rownames(Rho) <- colnames(Rho)
Rho


island_correlation_data <- expand.grid(x = cols, y = cols)
island_correlation_plot_df <- data.frame(
    x = island_correlation_data$x,
    y = island_correlation_data$y,
    z = apply(island_correlation_data, 1, function(row) Rho[row['x'], row['y']])
)
ggplot(island_correlation_plot_df, aes(x, y, fill = z)) +
    geom_tile()





psize <- d$logpop / max(d$logpop)
psize <- exp(psize * 1.5) - 2

island_position_df <- data.frame(lon = d$lon2, lat = d$lat, name = d$culture, psize = psize)
p <- ggplot() +
    geom_point(data = island_position_df, mapping = aes(lon, lat, size = psize)) +
    geom_text(data = island_position_df, mapping = aes(lon, lat, label = name), nudge_x = 2.5)

# now overlay the correlations
for (i in 1:10) {
    for (j in 1:10) {
        if (i < j) {
            df <- data.frame(x = d$lon2[i], y = d$lat[i], xend = d$lon2[j], yend = d$lat[j])
            p <- p + geom_segment(data = df, mapping = aes(x, y, xend = xend, yend = yend), linewidth = 2, alpha = Rho[i, j]^2)
        }
    }
}
print(p)





# Compute the posterior meduan relationship without distance
logpop.seq <- seq(from = 6, to = 14, length.out = 30)
post$bp
lambda <- sapply(logpop.seq, function(lp) exp(post$a + post$b * lp))
lambda.median <- apply(lambda, 2, median)
lambda.PI80 <- apply(lambda, 2, PI, prob = 0.8)

plot_df <- data.frame(
    logpop = d$logpop,
    tools = d$total_tools,
    name = d$culture
)

lines_df <- rbind(
    data.frame(
        logpop = logpop.seq,
        lambda = lambda.median,
        type = "Median"
    ),
    data.frame(
        logpop = logpop.seq,
        lambda = lambda.PI80[1, ],
        type = "Lower"
    ),
    data.frame(
        logpop = logpop.seq,
        lambda = lambda.PI80[2, ],
        type = "Upper"
    )
)

p <- ggplot() +
    geom_point(data = plot_df, mapping = aes(logpop, tools)) +
    geom_text(data = plot_df, mapping = aes(logpop, tools, label = name), nudge_x = 0.25) +
    geom_line(data = lines_df, mapping = aes(logpop, lambda, group = type)) +
    coord_cartesian(ylim = c(0, 70))

# overlay the correlations
for (i in 1:10) {
    for (j in i:10) {
        if (i < j) {
            df <- data.frame(x = d$logpop[i], y = d$total_tools[i], xend = d$logpop[j], yend = d$total_tools[j])
            p <- p + geom_segment(data = df, mapping = aes(x, y, xend = xend, yend = yend), alpha = Rho[i, j]^2, linewidth = 2)
        }
    }
}

print(p)





m14.8nc <- ulam(
    alist(
        T ~ dpois(lambda),
        lambda <- (a * P ^ b / g) * exp(k[society]),

        #non-centred Gaussian Process prior
        transpars> vector[10]: k <<- L_SIGMA * z,
        vector[10]: z ~ normal(0, 1),
        transpars> matrix[10, 10]: L_SIGMA <<- cholesky_decompose(SIGMA),
        transpars> matrix[10, 10]: SIGMA <- cov_GPL2(Dmat, etasq, rhosq, 0.01) ,

        c(a, b, g) ~ dexp(1),
        etasq ~ dexp(2),
        rhosq ~ dexp(0.5)
    ),
    data = dat_list,
    chains = 4,
    cores = 4,
    iter = 2000
)








par(bg = 'white')
brain_group_dag <- dagitty('dag {
    G1[latent, pos="0,0"]
    G2[pos="1,0"]
    B1[latent, pos="0,1"]
    B2[pos="1,1"]
    U1[latent, pos="0,2"]
    U2[latent, pos="1,2"]
    G1 -> G2
    G1 -> B2
    B1 -> B2
    U1 -> U2
    U1 -> B2
    U1 -> G2
}')
drawdag(brain_group_dag)





par(bg = 'white')
model_dag <- dagitty('dag {
P[pos="1,0"]
U[latent, pos="0,0"]
M[pos="0,-1"]
G[pos="-1,-1.5"]
B[pos="1,-1.5"]
P -> U -> M -> B <- U
M -> G <- U
G -> B
}')
drawdag(model_dag)





data(Primates301)
data(Primates301_nex)

par(bg = 'white')
plot(ladderize(Primates301_nex), type = 'fan', font = 1, no.margin = TRUE, label.offset = 1, cex = 0.5)





# get rid of the ones for which we don't have brain size, group size, and body size.
d <- Primates301
d$name <- as.character(d$name)
dstan <- d[complete.cases(d$group_size, d$body, d$brain), ]
spp_obs <- dstan$name

# we should have 151 species with complete observations
length(spp_obs)


dat_list <- list(
    N_spp = nrow(dstan),
    M = standardize(log(dstan$body)),
    B = standardize(log(dstan$brain)),
    G = standardize(log(dstan$group_size)),
    Imat = diag(nrow(dstan))
)

m14.9 <- ulam(
    alist(
        B ~ multi_normal(mu, SIGMA),
        mu <- a + bM * M + bG * G,
        matrix[N_spp, N_spp]: SIGMA <- Imat * sigma_sq,
        a ~ normal(0, 1),
        c(bM, bG) ~ normal(0, 0.5),
        sigma_sq ~ exponential(1)
    ),
    data = dat_list,
    chains = 4,
    cores = 4
)


par(bg = 'white')
precis(m14.9)
plot(precis(m14.9))





tree_trimmed <- keep.tip(Primates301_nex, spp_obs)
Rbm <- corBrownian(phy = tree_trimmed)
V <- vcv(Rbm)
Dmat <- cophenetic(tree_trimmed)
head(V)
head(Dmat)


species <- colnames(V)
species_pairs <- combn(species, 2) # all combinations of length 2
distances_df <- data.frame(
    species1 = species_pairs[1, ],
    species2 = species_pairs[2, ]
)
distances_df$phylogenetic_distance <- apply(distances_df, 1, function(row) { 
    s1 <- row['species1']
    s2 <- row['species2']
    distance <- Dmat[s1, s2]
    distance
})
distances_df$covariance = apply(distances_df, 1, function(row) V[row['species1'], row['species2']])

ggplot(distances_df, aes(phylogenetic_distance, covariance)) +
    geom_point()





# putting the species in the right order
dat_list$V <- V[spp_obs, spp_obs]

# convert to correlation matrix
dat_list$R <- dat_list$V / max(V)

# brownian motion model
m14.10 <- ulam(
    alist(
        B ~ multi_normal(mu, SIGMA),
        mu <- a + bM * M + bG * G,
        matrix[N_spp, N_spp]: SIGMA <- R * sigma_sq,
        a ~ normal(0, 1),
        c(bM, bG) ~ normal(0, 0.5),
        sigma_sq ~ exponential(1)
    ),
    data = dat_list,
    chains = 4,
    cores = 4
)


precis(m14.10)


par(bg = 'white')
plot(precis(m14.10))





# add scaled and reordered distance matrix
dat_list$Dmat <- Dmat[spp_obs, spp_obs] / max(Dmat)

m14.11 <- ulam(
    alist(
        B ~ multi_normal(mu, SIGMA),
        mu <- a + bM * M + bG * G,
        matrix[N_spp, N_spp]: SIGMA <- cov_GPL1(Dmat, etasq, rhosq, 0.01),
        a ~ normal(0, 1),
        c(bM, bG) ~ normal(0, 0.5),
        etasq ~ half_normal(1, 0.25),
        rhosq ~ half_normal(3, 0.25)
    ),
    data = dat_list,
    chains = 4,
    cores = 4
)


precis(m14.11)


par(bg = 'white')
plot(precis(m14.11))





post <- extract.samples(m14.11)

p <- ggplot()
phylo_distances <- seq(0, max(dat_list$Dmat), length.out = 100)

# posterior
for (i in 1:30) {
    cov <- post$etasq[i] * exp(-post$rhosq[i] * phylo_distances)
    p <- p + geom_line(data = data.frame(x = phylo_distances, y = cov, run = i), mapping = aes(x, y, group = run))
}

# prior
eta <- abs(rnorm(1e3, 1, 0.25))
rho <- abs(rnorm(1e3, 3, 0.25))
d_seq <- seq(from = 0, to = 1, length.out = 50)
K <- sapply(d_seq, function(x) eta * exp(-rho * x))

# mean and PI
prior_df <- data.frame(
    x = d_seq, 
    mean = colMeans(K),
    lower = apply(K, 2, PI)[1, ],
    upper = apply(K, 2, PI)[2, ]
)
p <- p + 
    geom_line(data = prior_df, mapping = aes(x, mean)) +
    geom_ribbon(data = prior_df, mapping = aes(x, ymin = lower, ymax = upper), alpha = 0.2)

print(p)





stancode(m14.11)




















get_simulated_robot_data <- function(rho) {
    a <- 3.5 # average morning wait times
    b <- -1 # average difference in the afternoon wait times
    sigma_a <- 1 # std dev in intercepts
    sigma_b <- 0.5 # std dev in slopes
    # rho <- -0.7 # correlation between intercepts and slopes

    Mu <- c(a, b)

    sigmas <- c(sigma_a, sigma_b) # standard deviations
    Rho <- matrix(c(1, rho, rho, 1), nrow = 2) # correlation matrix

    # matrix multiply to get the covariance matrix
    Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)

    # simulate the data
    N_cafes <- 20

    set.seed(5)
    vary_effects <- mvrnorm(N_cafes, Mu, Sigma)
    a_cafe <- vary_effects[, 1]
    b_cafe <- vary_effects[, 2]

    set.seed(22)
    N_visits <- 10
    afternoon <- rep(0:1, N_visits * N_cafes / 2)
    cafe_id <- rep(1:N_cafes, each = N_visits)
    mu <- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
    sigma <- 0.5 # sd within cafes
    wait <- rnorm(N_visits * N_cafes, mu, sigma)
    d <- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)
    d
}

fit_coffee_robot_model <- function(d) {
    # now let's fit the model!
    set.seed(867530)
    robot_model <- ulam(
        alist(
            wait ~ normal(mu, sigma),
            mu <- a_cafe[cafe] + b_cafe[cafe] * afternoon,
            c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
            a ~ normal(5, 2),
            b ~ normal(-1, 0.5),
            sigma_cafe ~ exponential(1),
            sigma ~ exponential(1),
            Rho ~ lkj_corr(2)
        ),
        data = d,
        chains = 4,
        cores = 4
    )
    post <- extract.samples(robot_model)
    post
}


rho.0.data <- get_simulated_robot_data(0)
rho.0.post <- fit_coffee_robot_model(rho.0.data)

rho.0.7.data <- get_simulated_robot_data(0.7)
rho.0.7.post <- fit_coffee_robot_model(rho.0.7.data)

prior_rho <- rlkjcorr(1e4, K = 2, eta = 2)

plot_df <- rbind(
    data.frame(points = rho.0.7.post$Rho[, 1, 2], type = 'Posterior - rho = 0.7'),
    data.frame(points = rho.0.post$Rho[, 1, 2], type = 'Posterior - rho = 0'),
    data.frame(points = prior_rho[, 1, 2], type = 'Prior')
)
ggplot(plot_df, aes(points, colour = type, group = type)) + 
    geom_density() + 
    scale_x_continuous(limits = c(-1, 1))








d <- get_simulated_robot_data(0.7)


# original version
set.seed(867530)
m14m2.original <- ulam(
    alist(
        wait ~ normal(mu, sigma),
        mu <- a_cafe[cafe] + b_cafe[cafe] * afternoon,
        c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a, b), Rho, sigma_cafe),
        a ~ normal(5, 2),
        b ~ normal(-1, 0.5),
        sigma_cafe ~ exponential(1),
        sigma ~ exponential(1),
        Rho ~ lkj_corr(2)
    ),
    data = d,
    chains = 4,
    cores = 4,
    log_lik = TRUE
)


m14m2.question <- ulam(
    alist(
        wait ~ normal(mu, sigma),
        mu <- a_cafe[cafe] + b_cafe[cafe] * afternoon,
        a_cafe[cafe] ~ normal(alpha, sigma_alpha),
        b_cafe[cafe] ~ normal(beta, sigma_beta),
        alpha ~ normal(0, 10),
        beta ~ normal(0, 10),
        sigma ~ exponential(1),
        sigma_alpha ~ exponential(1),
        sigma_beta ~ exponential(1)
    ),
    data = d,
    chains = 4,
    cores = 4,
    log_lik = TRUE
)


compare(m14m2.original, m14m2.question)








data(UCBadmit)
d <- UCBadmit
head(d)





d$dept_id <- as.integer(as.factor(d$dept))
d$G <- ifelse(d$applicant.gender == 'female', 1, 0)
d$N <- d$applications
d$a <- d$admit
model_data <- data.frame(
    dept_id = as.integer(as.factor(d$dept)),
    G = ifelse(d$applicant.gender == 'female', 1, 0),
    N = d$applications,
    a = d$admit
)

m14m3.centred <- ulam(
    alist(
        a ~ binomial(N, p),
        logit(p) <- alpha[dept_id] + beta[dept_id] * G,
        c(alpha, beta)[dept_id] ~ multi_normal(c(alpha_prior, beta_prior), Rho, sigma_dept),
        alpha_prior ~ normal(0, 1),
        beta_prior ~ normal(0, 1),
        sigma_dept ~ dexp(1),
        Rho ~ lkj_corr(2)
    ),
    data = model_data,
    chains = 4,
    cores = 4
)


# now for the non-centred version
m14m3.non_centred <- ulam(
    alist(
        a ~ binomial(N, p),
        # because now params is a matrix with a row for each department; the first column is alpha and the second is beta
        logit(p) <- params[dept_id, 1] + params[dept_id, 2] * G,
        
        # non-centred adaptive priors
        transpars> matrix[dept_id, 2]:params <- compose_noncentered(sigma_dept, L_Rho_dept, z_dept),
        matrix[2, dept_id]:z_dept ~ normal(0, 1),
        # c(alpha, beta)[dept_id] ~ multi_normal(c(alpha_prior, beta_prior), Rho, sigma_dept),
        # alpha_prior ~ normal(0, 1),
        # beta_prior ~ normal(0, 1),

        # fixed priors
        vector[2]:sigma_dept ~ dexp(1),
        cholesky_factor_corr[2]:L_Rho_dept ~ lkj_corr_cholesky(2),

        # now get the ordinary correlation matrix back
        gq> matrix[2, 2]:Rho_dept <<- Chol_to_Corr(L_Rho_dept)
    ),
    data = model_data,
    chains = 4,
    cores = 4
)





precis(m14m3.centred, depth = 3)
precis(m14m3.non_centred, depth = 3)








data(islandsDistMatrix)
data(Kline2)
d <- Kline2
d$society <- 1:10 # index observations
gp_dat_list <- list(
    T = d$total_tools,
    P = d$population,
    society = d$society,
    Dmat = islandsDistMatrix
)

# distances, measured in thousands of kilometres
Dmat <- islandsDistMatrix
# since this is a symmetrical matrix, we are just setting the column names to the short form of the island names...
colnames(Dmat) <- c("Ml", "Ti", "SC", "Ya", "Fi", "Tr", "Ch", "Mn", "To", "Ha")


chapter_11_dat_list <- list(
    T = d$total_tools,
    P = scale(log(d$population)),
    cid = ifelse(d$contact == "high", 2, 1)
)

# intercept only
m14m4.11.9 <- ulam(
    alist(
        T ~ dpois(lambda),
        log(lambda) <- a,
        a ~ dnorm(3, 0.5)
    ),
    data = chapter_11_dat_list,
    chains = 4,
    log_lik = TRUE
)

# interaction model
m14m4.11.10 <- ulam(
    alist(
        T ~ dpois(lambda),
        log(lambda) <- a[cid] + b[cid] * P,
        a[cid] ~ dnorm(3, 0.5),
        b[cid] ~ dnorm(0, 0.2)
    ),
    data = chapter_11_dat_list,
    chains = 4,
    log_lik = TRUE
)

m14m4.gp <- ulam(
    alist(
        T ~ dpois(lambda),
        lambda <- (a * P ^ b / g) * exp(k[society]),

        #non-centred Gaussian Process prior
        transpars> vector[10]: k <<- L_SIGMA * z,
        vector[10]: z ~ normal(0, 1),
        transpars> matrix[10, 10]: L_SIGMA <<- cholesky_decompose(SIGMA),
        transpars> matrix[10, 10]: SIGMA <- cov_GPL2(Dmat, etasq, rhosq, 0.01) ,

        c(a, b, g) ~ dexp(1),
        etasq ~ dexp(2),
        rhosq ~ dexp(0.5)
    ),
    data = gp_dat_list,
    chains = 4,
    cores = 4,
    iter = 2000,
    log_lik = TRUE
)


compare(m14m4.11.9, m14m4.11.10, m14m4.gp)





precis(m14m4.11.9, depth = 2)
precis(m14m4.11.10, depth = 2)
precis(m14m4.gp, depth = 3)








data(Primates301)
data(Primates301_nex)

d <- Primates301
d$name <- as.character(d$name)
dstan <- d[complete.cases(d$group_size, d$body, d$brain), ]
spp_obs <- dstan$name


dat_list <- list(
    N_spp = nrow(dstan),
    M = standardize(log(dstan$body)),
    B = standardize(log(dstan$brain)),
    G = standardize(log(dstan$group_size)),
    Imat = diag(nrow(dstan))
)

# change this so that it's the group size as the outcome and brain size as the predictor
# this is initially the one where we assume independence (no effect of phylogeny)
m14m5.1 <- ulam(
    alist(
        G ~ multi_normal(mu, SIGMA),
        mu <- a + bM * M + bB * B,
        matrix[N_spp, N_spp]: SIGMA <- Imat * sigma_sq,
        a ~ normal(0, 1),
        c(bM, bB) ~ normal(0, 0.5),
        sigma_sq ~ exponential(1)
    ),
    data = dat_list,
    chains = 4,
    cores = 4
)


precis(m14m5.1)
par(bg = 'white')
plot(precis(m14m5.1, depth = 3))





tree_trimmed <- keep.tip(Primates301_nex, spp_obs)
Rbm <- corBrownian(phy = tree_trimmed)
V <- vcv(Rbm)
Dmat <- cophenetic(tree_trimmed)

# putting the species in the right order
dat_list$V <- V[spp_obs, spp_obs]

# convert to correlation matrix
dat_list$R <- dat_list$V / max(V)

m14m5.2 <- ulam(
    alist(
        G ~ multi_normal(mu, SIGMA),
        mu <- a + bM * M + bB * B,
        matrix[N_spp, N_spp]: SIGMA <- R * sigma_sq,
        a ~ normal(0, 1),
        c(bM, bB) ~ normal(0, 0.5),
        sigma_sq ~ exponential(1)
    ),
    data = dat_list,
    chains = 4,
    cores = 4
)


pre <- precis(m14m5.2)
print(pre)
par(bg = 'white')
plot(pre)





# add scaled and reordered distance matrix
dat_list$Dmat <- Dmat[spp_obs, spp_obs] / max(Dmat)

m14m5.3 <- ulam(
    alist(
        G ~ multi_normal(mu, SIGMA),
        mu <- a + bM * M + bB * B,
        matrix[N_spp, N_spp]: SIGMA <- cov_GPL1(Dmat, etasq, rhosq, 0.01),
        a ~ normal(0, 1),
        c(bM, bB) ~ normal(0, 0.5),
        etasq ~ half_normal(1, 0.25),
        rhosq ~ half_normal(3, 0.25)
    ),
    data = dat_list,
    chains = 4,
    cores = 4
)


pre <- precis(m14m5.3, depth = 3)
print(pre)
par(bg = 'white')
plot(pre)






par(bg = 'white')
plot(coeftab(m14m5.3, m14m5.2, m14m5.1), pars = c("a", "bB", "bM"))








data(bangladesh)
d <- bangladesh

# there's an issue with the districts where one is missing for whatever reason, so we need to sort that out before we can use the district as an index variable
district_ids <- as.integer(as.factor(d$district))
d$district_id < - as.integer(as.factor(d$district))
head(d)





bangladesh_data <- list(
    district_id = district_ids,
    C = d$use.contraception,
    U = d$urban
)

m14h1.1 <- ulam(
    alist(
        C ~ dbinom(1, p),
        logit(p) <- alpha[district_id] + beta[district_id] * U,
        c(alpha, beta)[district_id] ~ multi_normal(c(a_bar, b_bar), Rho, Sigma),
        a_bar ~ normal(0, 1),
        b_bar ~ normal(0, 1),
        Rho ~ lkj_corr(2),
        Sigma ~ exponential(1)
    ),
    chains = 4,
    cores = 4,
    data = bangladesh_data
)


pre <- precis(m14h1.1, depth = 2)
print(pre)
par(bg = 'white')
plot(pre)





post <- extract.samples(m14h1.1)
sapply(post, head)


par(bg = 'white')
plot(precis(m14h1.1, depth = 3, pars = c("Rho", "Sigma")))





stancode(m14h1.1)





alpha_medians <- apply(post$alpha, 2, median)
beta_medians <- apply(post$beta, 2, median)
plot_df <- data.frame(
    alpha = alpha_medians,
    beta = beta_medians
)
ggplot(plot_df, aes(alpha, beta)) +
    geom_point()





# first get the proportion for the rural (basically just the inverse logit of the alpha parameter)
rural_ps <- inv_logit(post$alpha)
median_rural_proportions <- apply(rural_ps, 2, median)

# now the urban ones
urban_ps <- inv_logit(post$alpha + post$beta)
median_urban_proportions <- apply(urban_ps, 2, median)

# now plot them
plot_df <- data.frame(rural = median_rural_proportions, urban = median_urban_proportions)
ggplot(plot_df, aes(rural, urban))  +
    geom_point() +
    geom_segment(data = data.frame(x = 0, y = 0, xend = 1, yend = 1), aes(x, y, xend = xend, yend = yend))








par(bg = 'white')
dag.14h2 <- dagitty('dag{
Age
Children
Urban
District
Contraceptive
Age -> Contraceptive
Age -> Children
Children -> Contraceptive
Urban -> Contraceptive
District -> Contraceptive
}')
drawdag(dag.14h2)





bangladesh_data <- list(
    district_id = district_ids,
    C = d$use.contraception,
    U = d$urban,
    I = d$living.children,
    A = d$age.centered
)

m14h2.1 <- ulam(
    alist(
        C ~ dbinom(1, p),
        logit(p) <- alpha[district_id] + beta[district_id] * U + gamma * I + delta * A,
        c(alpha, beta)[district_id] ~ multi_normal(c(a_bar, b_bar), Rho, Sigma),
        c(gamma, delta) ~ normal(0, 1),
        c(a_bar, b_bar, g_bar, d_bar) ~ normal(0, 1),
        Rho ~ lkj_corr(2),
        Sigma ~ exponential(1)
    ),
    chains = 4,
    cores = 4,
    data = bangladesh_data
)


par(bg = 'white')
for (var in list("alpha", "beta", c("gamma", "delta"))) {
    pre <- precis(m14h2.1, depth = 3, pars = var)
    print(pre)
    plot(pre)
}








m14h2.2 <- ulam(
    alist(
        C ~ dbinom(1, p),
        logit(p) <- alpha[district_id] + beta[district_id] * U + gamma[district_id] * I + delta[district_id] * A,
        c(alpha, beta, gamma, delta)[district_id] ~ multi_normal(c(a_bar, b_bar, g_bar, d_bar), Rho, Sigma),
        c(a_bar, b_bar, g_bar, d_bar) ~ normal(0, 1),
        Rho ~ lkj_corr(2),
        Sigma ~ exponential(1)
    ),
    chains = 4,
    cores = 4,
    data = bangladesh_data
)


par(bg = 'white')
for (relevant_var in c("alpha", "beta", "gamma", "delta")) {
    pre <- precis(m14h2.2, depth = 3, pars = relevant_var)
    print(pre)
    plot(pre)
}








min(d$living.children)
max(d$living.children)





bangladesh_data$dirichlet_prior <- c(2, 2, 2)
bangladesh_data$I <- as.integer(bangladesh_data$I)
sapply(bangladesh_data, head)
m14h3.1 <- ulam(
    alist(
        C ~ dbinom(1, p),
        logit(p) <- alpha[district_id] + beta[district_id] * U + gamma * sum(epsilon_j[1:I]) + delta * A,
        c(alpha, beta)[district_id] ~ multi_normal(c(a_bar, b_bar), Rho, Sigma),
        c(gamma, delta) ~ normal(0, 1),
        c(a_bar, b_bar) ~ normal(0, 1),
        Rho ~ lkj_corr(2),
        Sigma ~ exponential(1),
        vector[4]: epsilon_j <<- append_row(0, epsilon),
        simplex[3]: epsilon ~ dirichlet(dirichlet_prior)
    ),
    chains = 4,
    cores = 4,
    data = bangladesh_data
)


par(bg = 'white')
pre <- precis(m14h3.1, depth = 3)
print(pre)
plot(pre)


par(bg = 'white')
pre <- precis(m14h3.1, depth = 3, pars = c("epsilon", "gamma"))
print(pre)
plot(pre)





m14h3.2 <- ulam(
    alist(
        C ~ dbinom(1, p),
        logit(p) <- alpha[district_id] + beta[district_id] * U + gamma[district_id] * sum(epsilon_j[1:I]) + delta[district_id] * A,
        c(alpha, beta, gamma, delta)[district_id] ~ multi_normal(c(a_bar, b_bar, g_bar, d_bar), Rho, Sigma),
        c(a_bar, b_bar, g_bar, d_bar) ~ normal(0, 1),
        Rho ~ lkj_corr(2),
        Sigma ~ exponential(1),
        vector[4]: epsilon_j <<- append_row(0, epsilon),
        simplex[3]: epsilon ~ dirichlet(dirichlet_prior)
    ),
    chains = 4,
    cores = 4,
    data = bangladesh_data
)


par(bg = 'white')
pre <- precis(m14h3.2, depth = 3)
print(pre)
plot(pre)


par(bg = 'white')
pre <- precis(m14h3.2, depth = 3, pars = c("epsilon", "gamma"))
print(pre)
plot(pre)








data(Oxboys)
d <- Oxboys
head(d)


ggplot(d, aes(age, height, group = Subject, colour = Subject)) +
    geom_point() +
    geom_line()





oxboys_data <- list(
    subject = d$Subject,
    A = d$age,
    H = d$height
)

m14h4 <- ulam(
    alist(
        H ~ normal(mu, sigma),
        mu <- a[subject] + b[subject] * A,
        c(a, b)[subject] ~ multi_normal(c(a_bar, b_bar), Rho, Sigma),
        Rho ~ lkj_corr(2),
        Sigma ~ exponential(1),
        a_bar ~ normal(150, 10),
        b_bar ~ normal(0, 1),
        sigma ~ exponential(1)
    ),
    data = oxboys_data,
    chains = 4,
    cores = 4
)


par(bg = 'white')
pre <- precis(m14h4, depth = 3)
print(pre)
plot(pre)


par(bg = 'white')
plot(precis(m14h4, depth = 3, pars="Sigma"))








par(bg = 'white')
pre <- precis(m14h4, depth = 3, pars = "Rho")
print(pre)
plot(pre)








post <- extract.samples(m14h4)

num_boys <- 10
ages <- seq(from = -1, to = 1, length.out = 20)

sigma <- mean(post$sigma)
a_bar <- mean(post$a_bar)
b_bar <- mean(post$b_bar)
mv_mean <- c(a_bar, b_bar)
sa <- mean(post$Sigma[, 1])
sb <- mean(post$Sigma[, 2])
rho <- mean(post$Rho[, 1, 2])
mv_sigma <- matrix(c(sa ^ 2, sa * sb * rho, sa * sb * rho, sb^2), nrow = 2)

simulated_data <- data.frame(subject = integer(), age = numeric(), height = numeric())

for (subject in 1:num_boys) {
    params <- rmvnorm(1, mean = mv_mean, sigma = mv_sigma)
    mu <- params[1] + params[2] * ages
    heights <- rnorm(length(mu), mu, sigma)
    subject_data <- data.frame(
        subject = subject,
        age = ages,
        height = heights
    )
    simulated_data <- rbind(
        simulated_data,
        subject_data
    )
}


ggplot(simulated_data, aes(age, height, group = subject, colour = subject)) +
    geom_point() +
    geom_line()



